#!/bin/bash
## FILENAME:  myjobsubmissionfile
#
#SBATCH -A mth210005       # allocation name
#SBATCH --ntasks=8    # Total # of MPI tasks
#SBATCH --time=24:00:00   # Total run time limit (hh:mm:ss)
#SBATCH -J myjobname     # Job name
#SBATCH -p shared     # Queue (partition) name
#SBATCH --mail-user=useremailaddress
#SBATCH--mail-type=all   # Send email to above address at begin and end of job

# Manage processing environment, load compilers, and applications.
ml restore pp
conda activate plumed_pytorch

. /anvil/projects/x-mth210005/Liyao/pytorch/torch/sourceme.sh 
. /anvil/projects/x-mth210005/Liyao/plumed/plumed2/sourceme.sh
. /anvil/projects/x-mth210005/Liyao/plumed/gromacslib/bin/GMXRC.bash
srun -n 8 gmx_mpi mdrun -s ../chi3.tpr -plumed plumed_phi3_psi3.dat -nsteps 25000000 -ntomp 1
plumed sum_hills --hills HILLS_phi3_psi3 --outfile fes_phi3_psi3.dat
