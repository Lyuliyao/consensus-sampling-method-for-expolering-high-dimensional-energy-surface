Restoring modules from user's pp
              :-) GROMACS - gmx grompp, 2023-plumed_2.9.0_dev (-:

Executable:   /anvil/projects/x-mth210005/Liyao/plumed/gromacslib/bin/gmx_mpi
Data prefix:  /anvil/projects/x-mth210005/Liyao/plumed/gromacslib
Working dir:  /anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus3
Command line:
  gmx_mpi grompp -o chi3_ref.tpr -c conf.gro -f grompp.mdp -maxwarn 3

Ignoring obsolete mdp entry 'optimize-fft'
Ignoring obsolete mdp entry 'ns-type'
Replacing old mdp entry 'verlet-buffer-drift' by 'verlet-buffer-tolerance'
Replacing old mdp entry 'nstxtcout' by 'nstxout-compressed'
Replacing old mdp entry 'xtc-precision' by 'compressed-x-precision'

NOTE 1 [file grompp.mdp]:
  rlist is equal to rvdw and/or rcoulomb: there is no explicit Verlet
  buffer. The cluster pair list does have a buffering effect, but choosing
  a larger rlist might be necessary for good energy conservation.


NOTE 2 [file grompp.mdp]:
  Setting nstcalcenergy (10000) equal to nstenergy (500)


WARNING 1 [file grompp.mdp]:
  You are generating velocities so I am assuming you are equilibrating a
  system. You are using Parrinello-Rahman pressure coupling, but this can
  be unstable for equilibration. If your system crashes, try equilibrating
  first with Berendsen pressure coupling. If you are not equilibrating the
  system, you can probably ignore this warning.

Generating 1-4 interactions: fudge = 1
Number of degrees of freedom in T-Coupling group Water is 5835.10
Number of degrees of freedom in T-Coupling group non-Water is 209.90

There were 2 NOTEs

There was 1 WARNING

Back Off! I just backed up chi3_ref.tpr to ./#chi3_ref.tpr.7#

GROMACS reminds you: "My Head Goes Pop Pop Pop Pop Pop" (F. Black)

Setting the LD random seed to -1090548027

Generated 98320 of the 98346 non-bonded parameter combinations

Generated 64935 of the 98346 1-4 parameter combinations

Excluding 3 bonded neighbours molecule type 'system1'

turning H bonds into constraints...

Excluding 2 bonded neighbours molecule type 'SOL'

turning H bonds into constraints...

Setting gen_seed to -632394500

Velocities were taken from a Maxwell distribution at 298 K
Analysing residue names:
There are:     5      Other residues
There are:   973      Water residues
Analysing residues not classified as Protein/DNA/RNA/Water and splitting into groups...

The largest distance between excluded atoms is 0.400 nm between atom 57 and 59
Calculating fourier grid dimensions for X Y Z
Using a fourier grid of 32x32x32, spacing 0.109 0.109 0.109

Estimate for the relative computational load of the PME mesh part: 0.27

This run will generate roughly 2 Mb of data
               :-) GROMACS - gmx mdrun, 2023-plumed_2.9.0_dev (-:

Executable:   /anvil/projects/x-mth210005/Liyao/plumed/gromacslib/bin/gmx_mpi
Data prefix:  /anvil/projects/x-mth210005/Liyao/plumed/gromacslib
Working dir:  /anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus3
Command line:
  gmx_mpi mdrun -s chi3_ref.tpr -plumed ./plumed1.dat -ntomp 1 -nsteps 100 -c conf.gro -nb cpu


Back Off! I just backed up md.log to ./#md.log.4#
Reading file chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Overriding nsteps with value passed on the command line: 100 steps, 0.1 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.4#

Back Off! I just backed up ener.edr to ./#ener.edr.4#
starting mdrun 'Generic title in water'
100 steps,      0.1 ps.

Writing final coordinates.

Back Off! I just backed up conf.gro to ./#conf.gro.4#

NOTE: 1 % of the run time was spent in domain decomposition,
      11 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:        0.326        0.326      100.0
                 (ns/day)    (hour/ns)
Performance:       26.748        0.897

GROMACS reminds you: "There is no place like ~" (Anonymous)

              :-) GROMACS - gmx grompp, 2023-plumed_2.9.0_dev (-:

Executable:   /anvil/projects/x-mth210005/Liyao/plumed/gromacslib/bin/gmx_mpi
Data prefix:  /anvil/projects/x-mth210005/Liyao/plumed/gromacslib
Working dir:  /anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus3
Command line:
  gmx_mpi grompp -o chi3_ref.tpr -c conf.gro -f grompp.mdp -maxwarn 3

Ignoring obsolete mdp entry 'optimize-fft'
Ignoring obsolete mdp entry 'ns-type'
Replacing old mdp entry 'verlet-buffer-drift' by 'verlet-buffer-tolerance'
Replacing old mdp entry 'nstxtcout' by 'nstxout-compressed'
Replacing old mdp entry 'xtc-precision' by 'compressed-x-precision'

NOTE 1 [file grompp.mdp]:
  rlist is equal to rvdw and/or rcoulomb: there is no explicit Verlet
  buffer. The cluster pair list does have a buffering effect, but choosing
  a larger rlist might be necessary for good energy conservation.


NOTE 2 [file grompp.mdp]:
  Setting nstcalcenergy (10000) equal to nstenergy (500)


WARNING 1 [file grompp.mdp]:
  You are generating velocities so I am assuming you are equilibrating a
  system. You are using Parrinello-Rahman pressure coupling, but this can
  be unstable for equilibration. If your system crashes, try equilibrating
  first with Berendsen pressure coupling. If you are not equilibrating the
  system, you can probably ignore this warning.

Generating 1-4 interactions: fudge = 1
Number of degrees of freedom in T-Coupling group Water is 5835.10
Number of degrees of freedom in T-Coupling group non-Water is 209.90

There were 2 NOTEs

There was 1 WARNING

Back Off! I just backed up chi3_ref.tpr to ./#chi3_ref.tpr.8#

GROMACS reminds you: "This work contains many things which are new and interesting. Unfortunately, everything that is new is not interesting, and everything which is interesting, is not new." (Lev Landau)

Setting the LD random seed to -18034961

Generated 98320 of the 98346 non-bonded parameter combinations

Generated 64935 of the 98346 1-4 parameter combinations

Excluding 3 bonded neighbours molecule type 'system1'

turning H bonds into constraints...

Excluding 2 bonded neighbours molecule type 'SOL'

turning H bonds into constraints...

Setting gen_seed to 904519551

Velocities were taken from a Maxwell distribution at 298 K
Analysing residue names:
There are:     5      Other residues
There are:   973      Water residues
Analysing residues not classified as Protein/DNA/RNA/Water and splitting into groups...

The largest distance between excluded atoms is 0.393 nm between atom 36 and 46
Calculating fourier grid dimensions for X Y Z
Using a fourier grid of 32x32x32, spacing 0.109 0.109 0.109

Estimate for the relative computational load of the PME mesh part: 0.27

This run will generate roughly 2 Mb of data
               :-) GROMACS - gmx mdrun, 2023-plumed_2.9.0_dev (-:

Executable:   /anvil/projects/x-mth210005/Liyao/plumed/gromacslib/bin/gmx_mpi
Data prefix:  /anvil/projects/x-mth210005/Liyao/plumed/gromacslib
Working dir:  /anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus3
Command line:
  gmx_mpi mdrun -s ../chi3_ref.tpr -c conf.gro -plumed ../plumed.dat -multidir ./esamble_1 ./esamble_2 ./esamble_3 ./esamble_4 ./esamble_5 ./esamble_6 ./esamble_7 ./esamble_8 ./esamble_9 ./esamble_10 ./esamble_11 ./esamble_12 ./esamble_13 ./esamble_14 ./esamble_15 ./esamble_16 ./esamble_17 ./esamble_18 ./esamble_19 ./esamble_20 ./esamble_21 ./esamble_22 ./esamble_23 ./esamble_24 ./esamble_25 ./esamble_26 ./esamble_27 ./esamble_28 ./esamble_29 ./esamble_30 ./esamble_31 ./esamble_32 ./esamble_33 ./esamble_34 ./esamble_35 ./esamble_36 ./esamble_37 ./esamble_38 ./esamble_39 ./esamble_40 ./esamble_41 ./esamble_42 ./esamble_43 ./esamble_44 ./esamble_45 ./esamble_46 ./esamble_47 ./esamble_48 ./esamble_49 ./esamble_50 ./esamble_51 ./esamble_52 ./esamble_53 ./esamble_54 ./esamble_55 ./esamble_56 ./esamble_57 ./esamble_58 ./esamble_59 ./esamble_60 ./esamble_61 ./esamble_62 ./esamble_63 ./esamble_64 -ntomp 1 -nsteps 5000000 -nb cpu


Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms



Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms


Can not increase nstlist because verlet-buffer-tolerance is not set or used
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

This is simulation 12 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

This is simulation 11 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

Using 8 MPI processes

This is simulation 13 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:


This is simulation 10 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
Using 8 MPI processes


This is simulation 20 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 17 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 2 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 16 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:


Using 8 MPI processes

This is simulation 4 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 21 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 0 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 1 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 3 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 5 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 6 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 7 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 9 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 14 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 15 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 8 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes


This is simulation 52 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 50 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes


This is simulation 51 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 47 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

This is simulation 25 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 53 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 56 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes


This is simulation 18 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 57 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 60 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
Using 8 MPI processes

This is simulation 19 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 61 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 22 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 48 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 46 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 49 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 27 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 54 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 26 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 62 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 23 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 28 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 29 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 58 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 30 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 55 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 59 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 31 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 63 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 24 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes


This is simulation 44 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 34 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

This is simulation 35 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 45 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 41 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 38 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 39 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 33 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 40 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 32 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 42 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 36 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 37 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 43 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

Using 8 MPI processes

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity


Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity


Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity


Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity


Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity
Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity
Non-default thread affinity set, disabling internal thread affinity
Non-default thread affinity set, disabling internal thread affinity
Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process


Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#
starting mdrun 'Generic title in water'

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#
5000000 steps,   5000.0 ps.

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'

Back Off! I just backed up ener.edr to ./#ener.edr.1#
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
4
4
4
4
4
4
4
4
0
0
0
0
0
0
0
0
11
11
11
11
11
11
11
11
12
12
12
12
12
12
6
6
6
6
6
6
6
6
9
9
9
9
9
9
9
9
12
12
2
2
2
2
2
2
2
2
8
8
8
8
8
8
8
8
10
10
10
10
10
10
10
10
1
1
1
1
1
1
56
1
47
3
47
3
3
47
3
3
3
5
47
5
5
5
14
14
14
23
14
14
14
47
15
56
15
15
15
23
15
15
56
15
56
15
47
56
1
3
61
3
61
23
5
61
5
61
5
61
47
5
50
7
50
7
50
7
50
7
23
50
7
50
7
50
7
47
50
7
38
13
52
38
13
38
52
13
38
52
13
52
38
13
33
23
52
13
52
13
52
13
52
14
23
56
14
23
60
33
23
17
60
26
33
60
33
26
60
33
30
60
30
33
60
33
30
60
33
30
60
38
30
30
61
38
30
61
38
30
61
34
17
62
34
17
62
17
62
17
62
17
62
34
17
62
17
34
62
25
62
34
25
63
34
25
63
34
25
63
34
25
63
40
25
63
40
63
40
40
63
25
25
26
40
63
26
40
51
40
51
40
26
26
51
26
43
26
51
43
21
51
43
51
43
21
21
51
43
21
21
51
43
53
43
21
21
43
53
53
46
21
46
53
46
53
22
46
22
22
53
22
22
46
22
53
22
46
53
46
54
22
24
46
54
24
39
24
54
39
54
39
24
24
54
39
24
54
24
39
54
39
24
54
39
20
55
39
20
20
55
42
20
55
42
42
55
20
20
42
55
42
20
55
42
55
42
55
20
27
27
42
27
27
57
45
57
27
45
57
45
27
57
27
45
27
57
45
28
57
45
45
57
31
31
31
45
57
32
31
58
31
32
58
32
58
32
31
31
31
58
32
16
58
32
58
32
16
16
58
32
16
16
58
35
16
59
16
59
35
16
35
59
35
18
18
18
59
35
35
59
35
59
35
18
18
18
59
44
59
44
18
48
44
18
28
44
48
44
48
44
28
48
28
28
44
28
48
28
44
48
36
48
36
28
29
29
48
36
49
36
29
49
36
49
36
29
29
29
29
49
36
29
49
36
49
37
19
19
37
19
49
19
37
49
37
37
19
19
19
37
19
37
37
56
41
56
41
41
41
41
41
41
41
