Restoring modules from user's pp
              :-) GROMACS - gmx grompp, 2023-plumed_2.9.0_dev (-:

Executable:   /anvil/projects/x-mth210005/Liyao/plumed/gromacslib/bin/gmx_mpi
Data prefix:  /anvil/projects/x-mth210005/Liyao/plumed/gromacslib
Working dir:  /anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus5
Command line:
  gmx_mpi grompp -o chi3_ref.tpr -c conf.gro -f grompp.mdp -maxwarn 3

Ignoring obsolete mdp entry 'optimize-fft'
Ignoring obsolete mdp entry 'ns-type'
Replacing old mdp entry 'verlet-buffer-drift' by 'verlet-buffer-tolerance'
Replacing old mdp entry 'nstxtcout' by 'nstxout-compressed'
Replacing old mdp entry 'xtc-precision' by 'compressed-x-precision'

NOTE 1 [file grompp.mdp]:
  rlist is equal to rvdw and/or rcoulomb: there is no explicit Verlet
  buffer. The cluster pair list does have a buffering effect, but choosing
  a larger rlist might be necessary for good energy conservation.


NOTE 2 [file grompp.mdp]:
  Setting nstcalcenergy (10000) equal to nstenergy (500)


WARNING 1 [file grompp.mdp]:
  You are generating velocities so I am assuming you are equilibrating a
  system. You are using Parrinello-Rahman pressure coupling, but this can
  be unstable for equilibration. If your system crashes, try equilibrating
  first with Berendsen pressure coupling. If you are not equilibrating the
  system, you can probably ignore this warning.

Generating 1-4 interactions: fudge = 1
Number of degrees of freedom in T-Coupling group Water is 5835.10
Number of degrees of freedom in T-Coupling group non-Water is 209.90

There were 2 NOTEs

There was 1 WARNING

Back Off! I just backed up chi3_ref.tpr to ./#chi3_ref.tpr.5#

GROMACS reminds you: "Courage is like - it's a habitus, a habit, a virtue: you get it by courageous acts. It's like you learn to swim by swimming. You learn courage by couraging." (Marie Daly)

Setting the LD random seed to -100692675

Generated 98320 of the 98346 non-bonded parameter combinations

Generated 64935 of the 98346 1-4 parameter combinations

Excluding 3 bonded neighbours molecule type 'system1'

turning H bonds into constraints...

Excluding 2 bonded neighbours molecule type 'SOL'

turning H bonds into constraints...

Setting gen_seed to -272646177

Velocities were taken from a Maxwell distribution at 298 K
Analysing residue names:
There are:     5      Other residues
There are:   973      Water residues
Analysing residues not classified as Protein/DNA/RNA/Water and splitting into groups...

The largest distance between excluded atoms is 0.391 nm between atom 8 and 31
Calculating fourier grid dimensions for X Y Z
Using a fourier grid of 32x32x32, spacing 0.108 0.108 0.108

Estimate for the relative computational load of the PME mesh part: 0.27

This run will generate roughly 2 Mb of data
               :-) GROMACS - gmx mdrun, 2023-plumed_2.9.0_dev (-:

Executable:   /anvil/projects/x-mth210005/Liyao/plumed/gromacslib/bin/gmx_mpi
Data prefix:  /anvil/projects/x-mth210005/Liyao/plumed/gromacslib
Working dir:  /anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus5
Command line:
  gmx_mpi mdrun -s chi3_ref.tpr -plumed ./plumed1.dat -ntomp 1 -nsteps 100 -c conf.gro -nb cpu


Back Off! I just backed up md.log to ./#md.log.3#
Reading file chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Overriding nsteps with value passed on the command line: 100 steps, 0.1 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.3#

Back Off! I just backed up ener.edr to ./#ener.edr.3#
starting mdrun 'Generic title in water'
100 steps,      0.1 ps.

Writing final coordinates.

Back Off! I just backed up conf.gro to ./#conf.gro.3#

NOTE: 1 % of the run time was spent in domain decomposition,
      10 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:        0.343        0.343      100.0
                 (ns/day)    (hour/ns)
Performance:       25.455        0.943

GROMACS reminds you: "The last good thing written in C was Franz Schubert's Symphony Number 9." (Erwin Dieterich)

              :-) GROMACS - gmx grompp, 2023-plumed_2.9.0_dev (-:

Executable:   /anvil/projects/x-mth210005/Liyao/plumed/gromacslib/bin/gmx_mpi
Data prefix:  /anvil/projects/x-mth210005/Liyao/plumed/gromacslib
Working dir:  /anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus5
Command line:
  gmx_mpi grompp -o chi3_ref.tpr -c conf.gro -f grompp.mdp -maxwarn 3

Ignoring obsolete mdp entry 'optimize-fft'
Ignoring obsolete mdp entry 'ns-type'
Replacing old mdp entry 'verlet-buffer-drift' by 'verlet-buffer-tolerance'
Replacing old mdp entry 'nstxtcout' by 'nstxout-compressed'
Replacing old mdp entry 'xtc-precision' by 'compressed-x-precision'

NOTE 1 [file grompp.mdp]:
  rlist is equal to rvdw and/or rcoulomb: there is no explicit Verlet
  buffer. The cluster pair list does have a buffering effect, but choosing
  a larger rlist might be necessary for good energy conservation.


NOTE 2 [file grompp.mdp]:
  Setting nstcalcenergy (10000) equal to nstenergy (500)


WARNING 1 [file grompp.mdp]:
  You are generating velocities so I am assuming you are equilibrating a
  system. You are using Parrinello-Rahman pressure coupling, but this can
  be unstable for equilibration. If your system crashes, try equilibrating
  first with Berendsen pressure coupling. If you are not equilibrating the
  system, you can probably ignore this warning.

Generating 1-4 interactions: fudge = 1
Number of degrees of freedom in T-Coupling group Water is 5835.10
Number of degrees of freedom in T-Coupling group non-Water is 209.90

There were 2 NOTEs

There was 1 WARNING

Back Off! I just backed up chi3_ref.tpr to ./#chi3_ref.tpr.6#

GROMACS reminds you: "It's Time to Move On" (F. Black)

Setting the LD random seed to 954118142

Generated 98320 of the 98346 non-bonded parameter combinations

Generated 64935 of the 98346 1-4 parameter combinations

Excluding 3 bonded neighbours molecule type 'system1'

turning H bonds into constraints...

Excluding 2 bonded neighbours molecule type 'SOL'

turning H bonds into constraints...

Setting gen_seed to -9961733

Velocities were taken from a Maxwell distribution at 298 K
Analysing residue names:
There are:     5      Other residues
There are:   973      Water residues
Analysing residues not classified as Protein/DNA/RNA/Water and splitting into groups...

The largest distance between excluded atoms is 0.394 nm between atom 62 and 70
Calculating fourier grid dimensions for X Y Z
Using a fourier grid of 32x32x32, spacing 0.108 0.108 0.108

Estimate for the relative computational load of the PME mesh part: 0.27

This run will generate roughly 2 Mb of data
               :-) GROMACS - gmx mdrun, 2023-plumed_2.9.0_dev (-:

Executable:   /anvil/projects/x-mth210005/Liyao/plumed/gromacslib/bin/gmx_mpi
Data prefix:  /anvil/projects/x-mth210005/Liyao/plumed/gromacslib
Working dir:  /anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus5
Command line:
  gmx_mpi mdrun -s ../chi3_ref.tpr -c conf.gro -plumed ../plumed.dat -multidir ./esamble_1 ./esamble_2 ./esamble_3 ./esamble_4 ./esamble_5 ./esamble_6 ./esamble_7 ./esamble_8 ./esamble_9 ./esamble_10 ./esamble_11 ./esamble_12 ./esamble_13 ./esamble_14 ./esamble_15 ./esamble_16 ./esamble_17 ./esamble_18 ./esamble_19 ./esamble_20 ./esamble_21 ./esamble_22 ./esamble_23 ./esamble_24 ./esamble_25 ./esamble_26 ./esamble_27 ./esamble_28 ./esamble_29 ./esamble_30 ./esamble_31 ./esamble_32 ./esamble_33 ./esamble_34 ./esamble_35 ./esamble_36 ./esamble_37 ./esamble_38 ./esamble_39 ./esamble_40 ./esamble_41 ./esamble_42 ./esamble_43 ./esamble_44 ./esamble_45 ./esamble_46 ./esamble_47 ./esamble_48 ./esamble_49 ./esamble_50 ./esamble_51 ./esamble_52 ./esamble_53 ./esamble_54 ./esamble_55 ./esamble_56 ./esamble_57 ./esamble_58 ./esamble_59 ./esamble_60 ./esamble_61 ./esamble_62 ./esamble_63 ./esamble_64 -ntomp 1 -nsteps 5000000 -nb cpu


Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#

Back Off! I just backed up md.log to ./#md.log.1#
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used



Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

This is simulation 60 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 58 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 61 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes


This is simulation 50 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 56 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 51 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 59 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:
This is simulation 28 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:



This is simulation 54 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 8 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
Using 8 MPI processes


This is simulation 57 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 62 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 9 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

This is simulation 55 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 48 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 52 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes


This is simulation 53 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 14 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 63 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 0 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 49 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 1 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 12 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
Using 8 MPI processes

This is simulation 13 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 5 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 10 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 15 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 2 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes


This is simulation 3 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 29 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 4 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 27 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 6 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 7 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 26 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

This is simulation 11 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 16 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 30 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
Using 8 MPI processes

This is simulation 17 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 31 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 20 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 21 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 18 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 22 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 23 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 24 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 19 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 25 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

Using 8 MPI processes

This is simulation 46 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 44 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 45 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 34 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 40 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes


This is simulation 35 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 36 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 41 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 32 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 38 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 33 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 37 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 43 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 42 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 39 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 47 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity





Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity
Non-default thread affinity set, disabling internal thread affinity
Non-default thread affinity set, disabling internal thread affinity
Non-default thread affinity set, disabling internal thread affinity
Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process
Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process




Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process




Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process


Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
3
3
3
3
3
3
3
3
13
13
13
13
13
13
13
13
2
2
2
2
2
2
2
2
6
6
6
6
6
56
6
56
6
56
6
56
7
56
7
56
7
56
7
56
7
32
7
7
7
32
1
1
32
1
1
1
1
1
1
12
12
32
12
12
12
12
12
32
12
11
58
11
58
32
58
11
58
51
11
29
11
11
32
11
32
11
43
15
43
15
43
15
43
15
43
15
43
43
29
15
43
15
44
15
44
9
44
9
44
51
9
44
51
29
9
44
51
9
44
55
44
9
55
41
9
55
41
9
41
55
8
41
55
0
29
41
55
8
41
55
8
41
55
8
41
58
0
42
0
58
42
58
0
42
58
0
42
62
0
42
62
0
42
62
0
42
62
8
42
62
8
46
29
62
8
46
29
8
46
62
29
46
62
29
10
28
10
46
51
10
46
51
28
10
51
46
28
10
51
46
50
28
10
39
50
39
28
10
39
63
28
10
39
63
28
4
39
50
28
4
39
50
16
4
39
50
16
4
39
50
16
4
40
50
16
4
40
50
16
4
40
4
63
16
40
63
5
16
40
5
40
16
63
20
5
40
5
20
40
5
45
20
45
5
20
63
45
5
20
45
5
20
45
14
20
45
48
14
20
45
48
14
21
45
48
14
21
33
14
48
21
33
14
48
21
33
14
48
26
33
14
48
26
33
48
26
33
53
33
26
53
26
33
26
34
53
26
34
53
34
26
53
34
21
53
34
21
53
34
21
53
34
21
54
34
23
54
35
23
63
35
23
63
35
23
49
35
23
35
49
23
35
49
23
35
49
23
35
49
36
25
49
36
25
36
49
25
49
36
25
25
36
52
25
36
52
36
52
25
36
25
52
52
37
22
52
37
22
37
52
22
52
37
54
22
37
22
54
37
54
22
37
22
37
54
54
38
54
57
38
38
22
38
57
57
24
38
57
27
57
27
38
27
38
57
27
27
38
27
47
30
57
57
30
47
30
47
59
30
47
59
59
30
47
47
59
30
59
59
30
47
30
47
59
59
17
60
17
60
60
17
60
17
60
17
60
17
60
17
60
17
18
61
61
18
61
18
61
61
18
18
61
61
18
61
18
18
19
19
19
19
19
19
19
19
24
24
24
24
24
24
24
27
27
31
31
31
31
31
31
31
31
