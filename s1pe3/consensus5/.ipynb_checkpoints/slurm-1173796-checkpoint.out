Restoring modules from user's pp
              :-) GROMACS - gmx grompp, 2023-plumed_2.9.0_dev (-:

Executable:   /anvil/projects/x-mth210005/Liyao/plumed/gromacslib/bin/gmx_mpi
Data prefix:  /anvil/projects/x-mth210005/Liyao/plumed/gromacslib
Working dir:  /anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus2
Command line:
  gmx_mpi grompp -o chi3_ref.tpr -c conf.gro -f grompp.mdp -maxwarn 3

Ignoring obsolete mdp entry 'optimize-fft'
Ignoring obsolete mdp entry 'ns-type'
Replacing old mdp entry 'verlet-buffer-drift' by 'verlet-buffer-tolerance'
Replacing old mdp entry 'nstxtcout' by 'nstxout-compressed'
Replacing old mdp entry 'xtc-precision' by 'compressed-x-precision'

NOTE 1 [file grompp.mdp]:
  rlist is equal to rvdw and/or rcoulomb: there is no explicit Verlet
  buffer. The cluster pair list does have a buffering effect, but choosing
  a larger rlist might be necessary for good energy conservation.


NOTE 2 [file grompp.mdp]:
  Setting nstcalcenergy (10000) equal to nstenergy (500)


WARNING 1 [file grompp.mdp]:
  You are generating velocities so I am assuming you are equilibrating a
  system. You are using Parrinello-Rahman pressure coupling, but this can
  be unstable for equilibration. If your system crashes, try equilibrating
  first with Berendsen pressure coupling. If you are not equilibrating the
  system, you can probably ignore this warning.

Generating 1-4 interactions: fudge = 1
Number of degrees of freedom in T-Coupling group Water is 5835.10
Number of degrees of freedom in T-Coupling group non-Water is 209.90

There were 2 NOTEs

There was 1 WARNING

Back Off! I just backed up chi3_ref.tpr to ./#chi3_ref.tpr.1#

GROMACS reminds you: "You got one part of that wrong. This is not meth." (Breaking Bad)

Setting the LD random seed to 2130444274

Generated 98320 of the 98346 non-bonded parameter combinations

Generated 64935 of the 98346 1-4 parameter combinations

Excluding 3 bonded neighbours molecule type 'system1'

turning H bonds into constraints...

Excluding 2 bonded neighbours molecule type 'SOL'

turning H bonds into constraints...

Setting gen_seed to -62914723

Velocities were taken from a Maxwell distribution at 298 K
Analysing residue names:
There are:     5      Other residues
There are:   973      Water residues
Analysing residues not classified as Protein/DNA/RNA/Water and splitting into groups...

The largest distance between excluded atoms is 0.393 nm between atom 16 and 20
Calculating fourier grid dimensions for X Y Z
Using a fourier grid of 32x32x32, spacing 0.109 0.109 0.109

Estimate for the relative computational load of the PME mesh part: 0.27

This run will generate roughly 2 Mb of data
               :-) GROMACS - gmx mdrun, 2023-plumed_2.9.0_dev (-:

Executable:   /anvil/projects/x-mth210005/Liyao/plumed/gromacslib/bin/gmx_mpi
Data prefix:  /anvil/projects/x-mth210005/Liyao/plumed/gromacslib
Working dir:  /anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus2
Command line:
  gmx_mpi mdrun -s chi3_ref.tpr -plumed ./plumed1.dat -ntomp 1 -nsteps 100 -c conf.gro -nb cpu


Back Off! I just backed up md.log to ./#md.log.1#
Reading file chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Overriding nsteps with value passed on the command line: 100 steps, 0.1 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.1#

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Generic title in water'
100 steps,      0.1 ps.

Writing final coordinates.

Back Off! I just backed up conf.gro to ./#conf.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:        0.351        0.351      100.0
                 (ns/day)    (hour/ns)
Performance:       24.876        0.965

GROMACS reminds you: "I love deadlines. I like the whooshing sound they make as they fly by." (Douglas Adams)

              :-) GROMACS - gmx grompp, 2023-plumed_2.9.0_dev (-:

Executable:   /anvil/projects/x-mth210005/Liyao/plumed/gromacslib/bin/gmx_mpi
Data prefix:  /anvil/projects/x-mth210005/Liyao/plumed/gromacslib
Working dir:  /anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus2
Command line:
  gmx_mpi grompp -o chi3_ref.tpr -c conf.gro -f grompp.mdp -maxwarn 3

Ignoring obsolete mdp entry 'optimize-fft'
Ignoring obsolete mdp entry 'ns-type'
Replacing old mdp entry 'verlet-buffer-drift' by 'verlet-buffer-tolerance'
Replacing old mdp entry 'nstxtcout' by 'nstxout-compressed'
Replacing old mdp entry 'xtc-precision' by 'compressed-x-precision'

NOTE 1 [file grompp.mdp]:
  rlist is equal to rvdw and/or rcoulomb: there is no explicit Verlet
  buffer. The cluster pair list does have a buffering effect, but choosing
  a larger rlist might be necessary for good energy conservation.


NOTE 2 [file grompp.mdp]:
  Setting nstcalcenergy (10000) equal to nstenergy (500)


WARNING 1 [file grompp.mdp]:
  You are generating velocities so I am assuming you are equilibrating a
  system. You are using Parrinello-Rahman pressure coupling, but this can
  be unstable for equilibration. If your system crashes, try equilibrating
  first with Berendsen pressure coupling. If you are not equilibrating the
  system, you can probably ignore this warning.

Generating 1-4 interactions: fudge = 1
Number of degrees of freedom in T-Coupling group Water is 5835.10
Number of degrees of freedom in T-Coupling group non-Water is 209.90

There were 2 NOTEs

There was 1 WARNING

Back Off! I just backed up chi3_ref.tpr to ./#chi3_ref.tpr.2#

GROMACS reminds you: "Suzy is a headbanger, her mother is a geek" (The Ramones)

Setting the LD random seed to -1145610370

Generated 98320 of the 98346 non-bonded parameter combinations

Generated 64935 of the 98346 1-4 parameter combinations

Excluding 3 bonded neighbours molecule type 'system1'

turning H bonds into constraints...

Excluding 2 bonded neighbours molecule type 'SOL'

turning H bonds into constraints...

Setting gen_seed to 1711258590

Velocities were taken from a Maxwell distribution at 298 K
Analysing residue names:
There are:     5      Other residues
There are:   973      Water residues
Analysing residues not classified as Protein/DNA/RNA/Water and splitting into groups...

The largest distance between excluded atoms is 0.396 nm between atom 30 and 43
Calculating fourier grid dimensions for X Y Z
Using a fourier grid of 32x32x32, spacing 0.109 0.109 0.109

Estimate for the relative computational load of the PME mesh part: 0.27

This run will generate roughly 2 Mb of data
               :-) GROMACS - gmx mdrun, 2023-plumed_2.9.0_dev (-:

Executable:   /anvil/projects/x-mth210005/Liyao/plumed/gromacslib/bin/gmx_mpi
Data prefix:  /anvil/projects/x-mth210005/Liyao/plumed/gromacslib
Working dir:  /anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus2
Command line:
  gmx_mpi mdrun -s ../chi3_ref.tpr -c conf.gro -plumed ../plumed.dat -multidir ./esamble_1 ./esamble_2 ./esamble_3 ./esamble_4 ./esamble_5 ./esamble_6 ./esamble_7 ./esamble_8 ./esamble_9 ./esamble_10 ./esamble_11 ./esamble_12 ./esamble_13 ./esamble_14 ./esamble_15 ./esamble_16 ./esamble_17 ./esamble_18 ./esamble_19 ./esamble_20 ./esamble_21 ./esamble_22 ./esamble_23 ./esamble_24 ./esamble_25 ./esamble_26 ./esamble_27 ./esamble_28 ./esamble_29 ./esamble_30 ./esamble_31 ./esamble_32 ./esamble_33 ./esamble_34 ./esamble_35 ./esamble_36 ./esamble_37 ./esamble_38 ./esamble_39 ./esamble_40 ./esamble_41 ./esamble_42 ./esamble_43 ./esamble_44 ./esamble_45 ./esamble_46 ./esamble_47 ./esamble_48 ./esamble_49 ./esamble_50 ./esamble_51 ./esamble_52 ./esamble_53 ./esamble_54 ./esamble_55 ./esamble_56 ./esamble_57 ./esamble_58 ./esamble_59 ./esamble_60 ./esamble_61 ./esamble_62 ./esamble_63 ./esamble_64 -ntomp 1 -nsteps 1000000 -nb cpu


Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#

Back Off! I just backed up md.log to ./#md.log.2#
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 1000000 steps, 1e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

This is simulation 9 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes


This is simulation 26 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

This is simulation 62 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 63 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 53 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 52 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

This is simulation 50 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 51 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

Using 8 MPI processes


This is simulation 8 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

This is simulation 58 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 59 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 60 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes


Using 8 MPI processes
Using 8 MPI processes

This is simulation 27 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 16 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

This is simulation 54 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 57 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 56 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 61 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 49 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes


This is simulation 55 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 48 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
Using 8 MPI processes


This is simulation 24 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes


This is simulation 25 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 2 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 6 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 17 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

This is simulation 7 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 3 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

This is simulation 18 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 4 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 30 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 12 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 20 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 29 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes


This is simulation 31 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 0 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 28 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 13 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 21 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 19 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 1 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 22 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 5 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 23 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 14 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 10 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
Using 8 MPI processes

This is simulation 11 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
Using 8 MPI processes

This is simulation 15 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 34 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes


This is simulation 33 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 32 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:


This is simulation 35 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
Using 8 MPI processes


This is simulation 37 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 38 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 39 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 36 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

This is simulation 44 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 46 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 47 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 40 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 41 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 42 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 43 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 45 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

Using 8 MPI processes

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity


Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity


Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity


Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity


Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity
Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity
Non-default thread affinity set, disabling internal thread affinity
Non-default thread affinity set, disabling internal thread affinity
Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process


Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#
starting mdrun 'Generic title in water'

Back Off! I just backed up ener.edr to ./#ener.edr.2#
1000000 steps,   1000.0 ps.

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'

Back Off! I just backed up ener.edr to ./#ener.edr.2#
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.

Back Off! I just backed up ener.edr to ./#ener.edr.2#
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
starting mdrun 'Generic title in water'
1000000 steps,   1000.0 ps.
5
21
21
21
21
21
5
21
21
21
5
5
5
49
5
49
5
5
49
10
10
10
10
10
4
4
4
4
4
4
4
49
4
7
7
7
60
7
7
7
7
7
60
10
10
10
12
12
30
12
60
12
12
12
12
12
6
33
6
30
6
33
6
60
6
6
33
6
6
33
13
33
13
13
63
13
13
33
49
13
49
30
13
49
30
13
49
30
0
58
30
58
0
30
58
0
30
58
0
19
58
0
19
58
0
19
58
0
19
58
0
33
19
9
60
19
9
60
19
60
9
19
60
9
20
62
9
20
62
9
20
62
9
20
62
9
20
62
14
20
14
62
20
33
14
62
20
62
14
25
63
14
25
63
14
25
63
14
25
63
25
14
63
25
1
63
25
63
1
25
59
1
59
26
1
26
59
34
1
26
59
1
34
26
59
1
34
26
59
1
26
34
59
8
26
34
59
8
50
26
34
8
50
29
34
8
50
29
34
8
50
29
38
8
50
29
8
38
51
29
8
51
38
29
11
38
51
29
11
38
51
29
11
38
51
31
11
51
31
38
11
51
31
38
51
11
31
36
50
11
31
50
11
36
31
50
2
36
31
57
2
36
31
57
2
36
17
57
2
36
17
2
57
36
17
2
61
17
2
36
61
17
2
61
35
17
61
3
35
17
61
3
35
61
17
3
35
61
22
3
35
61
22
3
35
52
22
3
35
52
3
24
35
52
3
24
52
15
41
24
52
15
41
24
52
15
41
24
52
15
41
24
52
15
41
24
53
15
24
41
53
15
28
41
53
15
28
53
41
28
54
45
54
45
54
45
28
54
45
54
45
28
28
54
28
28
45
55
16
55
45
16
55
16
45
16
55
47
16
16
47
55
55
16
47
55
16
47
57
18
57
47
18
57
47
18
47
57
18
47
48
18
32
18
48
18
32
48
32
18
48
32
48
22
48
32
22
48
22
32
48
22
32
53
22
32
53
23
23
37
53
23
37
53
54
23
37
54
23
37
55
23
23
37
56
37
37
23
27
37
56
27
39
39
56
27
56
39
56
27
39
56
56
27
56
39
27
39
27
39
27
39
46
46
46
46
46
46
46
46
40
40
40
40
40
40
40
40
42
42
42
42
42
42
42
42
43
43
43
43
43
43
43
43
44
44
44
44
44
44
44
44

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Writing final coordinates.

Writing final coordinates.

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Writing final coordinates.

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Writing final coordinates.

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Writing final coordinates.

Writing final coordinates.

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Writing final coordinates.

Writing final coordinates.

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#

Back Off! I just backed up conf.gro to ./#conf.gro.1#


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.



Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.



Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.



Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.1%.
 The balanceable part of the MD step is 42%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.7%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.



Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.1%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.7%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 42%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.7%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      13 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 42%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.8%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 42%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      13 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 42%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 42%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.7%.


NOTE: 3 % of the run time was spent in domain decomposition,
      13 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %




NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)



Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

GROMACS reminds you: "Science is a way of thinking much more than it is a body of knowledge." (Carl Sagan)

Submitted batch job 1173798
JobID                     Submit  Partition    CPUTime    Elapsed     MaxRSS        NodeList  ReqCPUS 
------------ ------------------- ---------- ---------- ---------- ---------- --------------- -------- 
1173796      2023-02-26T09:03:03  wholenode 4-10:57:04   00:12:32            a[398-399,406,+      512 
1173796.bat+ 2023-02-26T09:03:03            1-02:44:16   00:12:32                       a398      128 
1173796.ext+ 2023-02-26T09:03:03            4-10:57:04   00:12:32            a[398-399,406,+      512 
1173796.0    2023-02-26T09:03:10            4-09:40:16   00:12:23    198.48M a[398-399,406,+      512 
