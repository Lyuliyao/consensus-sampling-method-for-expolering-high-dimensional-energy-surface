Restoring modules from user's pp
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus2/md_file.py", line 21, in generate_plumed_file
    """.format(id-1,','.join(str(a) for a in X[:])),file=f)
NameError: name 'X' is not defined
              :-) GROMACS - gmx grompp, 2023-plumed_2.9.0_dev (-:

Executable:   /anvil/projects/x-mth210005/Liyao/plumed/gromacslib/bin/gmx_mpi
Data prefix:  /anvil/projects/x-mth210005/Liyao/plumed/gromacslib
Working dir:  /anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus2
Command line:
  gmx_mpi grompp -o chi3_ref.tpr -c conf.gro -f grompp.mdp -maxwarn 3

Ignoring obsolete mdp entry 'optimize-fft'
Ignoring obsolete mdp entry 'ns-type'
Replacing old mdp entry 'verlet-buffer-drift' by 'verlet-buffer-tolerance'
Replacing old mdp entry 'nstxtcout' by 'nstxout-compressed'
Replacing old mdp entry 'xtc-precision' by 'compressed-x-precision'

NOTE 1 [file grompp.mdp]:
  rlist is equal to rvdw and/or rcoulomb: there is no explicit Verlet
  buffer. The cluster pair list does have a buffering effect, but choosing
  a larger rlist might be necessary for good energy conservation.


NOTE 2 [file grompp.mdp]:
  Setting nstcalcenergy (10000) equal to nstenergy (500)


WARNING 1 [file grompp.mdp]:
  You are generating velocities so I am assuming you are equilibrating a
  system. You are using Parrinello-Rahman pressure coupling, but this can
  be unstable for equilibration. If your system crashes, try equilibrating
  first with Berendsen pressure coupling. If you are not equilibrating the
  system, you can probably ignore this warning.

Generating 1-4 interactions: fudge = 1
Number of degrees of freedom in T-Coupling group Water is 5835.10
Number of degrees of freedom in T-Coupling group non-Water is 209.90

There were 2 NOTEs

There was 1 WARNING

Back Off! I just backed up chi3_ref.tpr to ./#chi3_ref.tpr.7#

GROMACS reminds you: "Insane In Tha Membrane" (Cypress Hill)

Setting the LD random seed to -1544643745

Generated 98320 of the 98346 non-bonded parameter combinations

Generated 64935 of the 98346 1-4 parameter combinations

Excluding 3 bonded neighbours molecule type 'system1'

turning H bonds into constraints...

Excluding 2 bonded neighbours molecule type 'SOL'

turning H bonds into constraints...

Setting gen_seed to -71567875

Velocities were taken from a Maxwell distribution at 298 K
Analysing residue names:
There are:     5      Other residues
There are:   973      Water residues
Analysing residues not classified as Protein/DNA/RNA/Water and splitting into groups...

The largest distance between excluded atoms is 0.396 nm between atom 31 and 54
Calculating fourier grid dimensions for X Y Z
Using a fourier grid of 32x32x32, spacing 0.108 0.108 0.108

Estimate for the relative computational load of the PME mesh part: 0.27

This run will generate roughly 2 Mb of data
               :-) GROMACS - gmx mdrun, 2023-plumed_2.9.0_dev (-:

Executable:   /anvil/projects/x-mth210005/Liyao/plumed/gromacslib/bin/gmx_mpi
Data prefix:  /anvil/projects/x-mth210005/Liyao/plumed/gromacslib
Working dir:  /anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus2
Command line:
  gmx_mpi mdrun -s chi3_ref.tpr -plumed ./plumed1.dat -ntomp 1 -nsteps 100 -c conf.gro -nb cpu


Back Off! I just backed up md.log to ./#md.log.4#
Reading file chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Overriding nsteps with value passed on the command line: 100 steps, 0.1 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread 


Back Off! I just backed up traj_comp.xtc to ./#traj_comp.xtc.4#

Back Off! I just backed up ener.edr to ./#ener.edr.4#
starting mdrun 'Generic title in water'
100 steps,      0.1 ps.

Writing final coordinates.

Back Off! I just backed up conf.gro to ./#conf.gro.4#

NOTE: 1 % of the run time was spent in domain decomposition,
      13 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

               Core t (s)   Wall t (s)        (%)
       Time:        0.271        0.271      100.0
                 (ns/day)    (hour/ns)
Performance:       32.228        0.745

GROMACS reminds you: "What you do makes a difference, and you have to decide what kind of difference you want to make." (Jane Goodall)

              :-) GROMACS - gmx grompp, 2023-plumed_2.9.0_dev (-:

Executable:   /anvil/projects/x-mth210005/Liyao/plumed/gromacslib/bin/gmx_mpi
Data prefix:  /anvil/projects/x-mth210005/Liyao/plumed/gromacslib
Working dir:  /anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus2
Command line:
  gmx_mpi grompp -o chi3_ref.tpr -c conf.gro -f grompp.mdp -maxwarn 3

Ignoring obsolete mdp entry 'optimize-fft'
Ignoring obsolete mdp entry 'ns-type'
Replacing old mdp entry 'verlet-buffer-drift' by 'verlet-buffer-tolerance'
Replacing old mdp entry 'nstxtcout' by 'nstxout-compressed'
Replacing old mdp entry 'xtc-precision' by 'compressed-x-precision'

NOTE 1 [file grompp.mdp]:
  rlist is equal to rvdw and/or rcoulomb: there is no explicit Verlet
  buffer. The cluster pair list does have a buffering effect, but choosing
  a larger rlist might be necessary for good energy conservation.


NOTE 2 [file grompp.mdp]:
  Setting nstcalcenergy (10000) equal to nstenergy (500)


WARNING 1 [file grompp.mdp]:
  You are generating velocities so I am assuming you are equilibrating a
  system. You are using Parrinello-Rahman pressure coupling, but this can
  be unstable for equilibration. If your system crashes, try equilibrating
  first with Berendsen pressure coupling. If you are not equilibrating the
  system, you can probably ignore this warning.

Generating 1-4 interactions: fudge = 1
Number of degrees of freedom in T-Coupling group Water is 5835.10
Number of degrees of freedom in T-Coupling group non-Water is 209.90

There were 2 NOTEs

There was 1 WARNING

Back Off! I just backed up chi3_ref.tpr to ./#chi3_ref.tpr.8#

GROMACS reminds you: "It's Calling Me to Break my Bonds, Again..." (Van der Graaf)

Setting the LD random seed to -1025

Generated 98320 of the 98346 non-bonded parameter combinations

Generated 64935 of the 98346 1-4 parameter combinations

Excluding 3 bonded neighbours molecule type 'system1'

turning H bonds into constraints...

Excluding 2 bonded neighbours molecule type 'SOL'

turning H bonds into constraints...

Setting gen_seed to 1064041471

Velocities were taken from a Maxwell distribution at 298 K
Analysing residue names:
There are:     5      Other residues
There are:   973      Water residues
Analysing residues not classified as Protein/DNA/RNA/Water and splitting into groups...

The largest distance between excluded atoms is 0.399 nm between atom 34 and 62
Calculating fourier grid dimensions for X Y Z
Using a fourier grid of 32x32x32, spacing 0.108 0.108 0.108

Estimate for the relative computational load of the PME mesh part: 0.27

This run will generate roughly 2 Mb of data
               :-) GROMACS - gmx mdrun, 2023-plumed_2.9.0_dev (-:

Executable:   /anvil/projects/x-mth210005/Liyao/plumed/gromacslib/bin/gmx_mpi
Data prefix:  /anvil/projects/x-mth210005/Liyao/plumed/gromacslib
Working dir:  /anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus2
Command line:
  gmx_mpi mdrun -s ../chi3_ref.tpr -c conf.gro -plumed ../plumed.dat -multidir ./esamble_1 ./esamble_2 ./esamble_3 ./esamble_4 ./esamble_5 ./esamble_6 ./esamble_7 ./esamble_8 ./esamble_9 ./esamble_10 ./esamble_11 ./esamble_12 ./esamble_13 ./esamble_14 ./esamble_15 ./esamble_16 ./esamble_17 ./esamble_18 ./esamble_19 ./esamble_20 ./esamble_21 ./esamble_22 ./esamble_23 ./esamble_24 ./esamble_25 ./esamble_26 ./esamble_27 ./esamble_28 ./esamble_29 ./esamble_30 ./esamble_31 ./esamble_32 ./esamble_33 ./esamble_34 ./esamble_35 ./esamble_36 ./esamble_37 ./esamble_38 ./esamble_39 ./esamble_40 ./esamble_41 ./esamble_42 ./esamble_43 ./esamble_44 ./esamble_45 ./esamble_46 ./esamble_47 ./esamble_48 ./esamble_49 ./esamble_50 ./esamble_51 ./esamble_52 ./esamble_53 ./esamble_54 ./esamble_55 ./esamble_56 ./esamble_57 ./esamble_58 ./esamble_59 ./esamble_60 ./esamble_61 ./esamble_62 ./esamble_63 ./esamble_64 -ntomp 1 -nsteps 5000000 -nb cpu

Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Reading file ../chi3_ref.tpr, VERSION 2023-plumed_2.9.0_dev (single precision)
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

This is simulation 0 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 1 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms


This is simulation 26 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 16 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 22 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 25 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 27 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 23 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 28 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 30 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 18 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 19 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 17 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes


This is simulation 10 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 11 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 3 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 4 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 5 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 7 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 2 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 9 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 12 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 13 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 14 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 15 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 8 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 6 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Can not increase nstlist because verlet-buffer-tolerance is not set or used
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms


This is simulation 47 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 38 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 39 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 40 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 42 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 43 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 41 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 45 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 32 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 33 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 44 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Overriding nsteps with value passed on the command line: 5000000 steps, 5e+03 ps

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Can not increase nstlist because verlet-buffer-tolerance is not set or used

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms


Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms
Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms

Update groups can not be used for this system because atoms that are (in)directly constrained together are interdispersed with other atoms


This is simulation 59 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 56 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

This is simulation 58 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes


This is simulation 57 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
Using 8 MPI processes

This is simulation 62 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes


This is simulation 51 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 54 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 60 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 50 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 53 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 49 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity


Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity
Non-default thread affinity set, disabling internal thread affinity

This is simulation 31 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 29 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 20 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 21 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 24 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity


Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity
Non-default thread affinity set, disabling internal thread affinity

This is simulation 36 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 34 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 35 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 37 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 46 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity


Non-default thread affinity set, disabling internal thread affinity
Non-default thread affinity set, disabling internal thread affinity

This is simulation 61 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 55 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 48 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

This is simulation 52 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes
This is simulation 63 out of 64 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 8 MPI processes

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity

Non-default thread affinity set, disabling internal thread affinity


Non-default thread affinity set, disabling internal thread affinity
Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process




Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process

Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process



Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process


Using 1 OpenMP thread per MPI process

starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.
starting mdrun 'Generic title in water'
5000000 steps,   5000.0 ps.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.

Writing final coordinates.


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 39%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.1%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.7%.


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 39%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.



Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.



Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.1%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.2%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.7%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.2%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.7%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %



Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.1%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.7%.



Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.1%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 39%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.5%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.3%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.7%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 39%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.1%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 42%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.1%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.1%.
 The balanceable part of the MD step is 39%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.1%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 39%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.



Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.1%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.7%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.1%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 39%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.7%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.1%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.7%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.7%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

GROMACS reminds you: "He's using code that only you and I know" (Kate Bush)



Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 41%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 39%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)



Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 39%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.5%.


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)



Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 3.9%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)



Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.0%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


Dynamic load balancing report:
 DLB was turned on during the run due to measured imbalance.
 Average load imbalance: 4.1%.
 The balanceable part of the MD step is 40%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 % Y 0 %


NOTE: 4 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)


NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)

NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)
NOTE: 3 % of the run time was spent in domain decomposition,
      14 % of the run time was spent in pair search,
      you might want to increase nstlist (this has no effect on accuracy)
rm: cannot remove './esamble_*/#*': No such file or directory
cp: cannot stat './esamble_1/mean.txt': No such file or directory
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/anvil/projects/x-mth210005/Liyao/consensus/chi3_ref/consensus2/post_processing.py", line 11, in collect_file
    data = np.loadtxt(file_list[0])
IndexError: list index out of range
Submitted batch job 1171943
JobID                     Submit  Partition    CPUTime    Elapsed     MaxRSS        NodeList  ReqCPUS 
------------ ------------------- ---------- ---------- ---------- ---------- --------------- -------- 
1171777      2023-02-23T23:24:59  wholenode 20-18:20:48   00:58:24                 a[898-901]      512 
1171777.bat+ 2023-02-23T23:24:59            5-04:35:12   00:58:24                       a898      128 
1171777.ext+ 2023-02-23T23:24:59            20-18:20:48   00:58:24                 a[898-901]      512 
1171777.0    2023-02-23T23:25:04            20-17:29:36   00:58:18    173.19M      a[898-901]      512 
